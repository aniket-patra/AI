{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ACMUU1Da5CaIc4AjxYbcDiydxH41dZP0",
      "authorship_tag": "ABX9TyOmHOzcr4o3ycpGOVjeSRUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniket-patra/AI/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw3f0eUgMKoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azh2ijthMNai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfz2yOJMbZM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzyECqhRMb1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "d3995878-0fc2-4716-fefa-adaaec5c9abe"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "\n",
        "import re\n",
        "\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# function for text cleaning \n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    # remove backslash-apostrophe \n",
        "\n",
        "    text = re.sub(\"\\'\", \"\", text) \n",
        "\n",
        "    # remove everything except alphabets \n",
        "\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
        "\n",
        "    # remove whitespaces \n",
        "\n",
        "    text = ' '.join(text.split()) \n",
        "\n",
        "    # convert text to lowercase \n",
        "\n",
        "    text = text.lower()     \n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# function for word frequency\n",
        "    \n",
        "def freq_words(x, terms = 30): \n",
        "\n",
        "  all_words = ' '.join([text for text in x]) \n",
        "\n",
        "  all_words = all_words.split() \n",
        "\n",
        "  fdist = nltk.FreqDist(all_words) \n",
        "\n",
        "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})   \n",
        "\n",
        "  # selecting top 20 most frequent words \n",
        "\n",
        "  d = words_df.nlargest(columns=\"count\", n = terms)   \n",
        "\n",
        "  # visualize words and frequencies\n",
        "\n",
        "  plt.figure(figsize=(12,15)) \n",
        "\n",
        "  ax = sns.barplot(data=d, x= \"count\", y = \"word\") \n",
        "\n",
        "  ax.set(ylabel = 'Word') \n",
        "\n",
        "  plt.show()  \n",
        "  \n",
        "# function to remove stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "\n",
        "#inference function\n",
        "def infer_tags(q):\n",
        "\n",
        "    q = clean_text(q)\n",
        "\n",
        "    q = remove_stopwords(q)\n",
        "\n",
        "    q_vec = tfidf_vectorizer.transform([q])\n",
        "\n",
        "    q_pred = clf.predict(q_vec)\n",
        "\n",
        "    return multilabel_binarizer.inverse_transform(q_pred)\n",
        "\n",
        "path=\"/content/drive/My Drive/Data/book.txt\"\n",
        "\n",
        "data = pd.read_csv(path, sep=\"\\t\", header=None)\n",
        "data.columns = [\"Wikipedia_ID\",\"Freebase ID\",\"Book_title\",\"Author\",\"Publication date\",\"Book_genres\",\"Plot\"]\n",
        "#data.fillna(0, inplace=True)    #used to fill NaN with some other value\n",
        "#data.to_csv('book.tsv', sep = '\\t')\n",
        "\n",
        "data.drop(columns=[\"Freebase ID\",\"Author\",\"Publication date\"], inplace=True) #Dropping unneccesary attributes\n",
        "data.dropna(subset = [\"Book_genres\",\"Wikipedia_ID\"], inplace=True)    #dropping rows with NaN values\n",
        "#data.to_csv('book.tsv', sep = '\\t')\n",
        "\n",
        "genres = []\n",
        "\n",
        "# extract genres\n",
        "for i in data['Book_genres']: \n",
        "  genres.append(list(json.loads(i).values())) \n",
        "\n",
        "\n",
        "\n",
        "# add to 'data' dataframe  \n",
        "data['genre_new'] = genres\n",
        "\n",
        "all_genres = sum(genres,[])  #total number of genres\n",
        "#len(set(all_genres))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "all_genres = nltk.FreqDist(all_genres)    #for plotting genres\n",
        "\n",
        "# create dataframe\n",
        "\n",
        "all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), \n",
        "\n",
        "                              'Count': list(all_genres.values())})\n",
        "\n",
        "g = all_genres_df.nlargest(columns=\"Count\", n = 50) \n",
        "\n",
        "plt.figure(figsize=(12,15)) \n",
        "\n",
        "ax = sns.barplot(data=g, x= \"Count\", y = \"Genre\") \n",
        "\n",
        "ax.set(ylabel = 'Count') \n",
        "plt.show() \"\"\"\n",
        "\n",
        "\n",
        "data['clean_plot']=data['Plot'].apply(lambda x: clean_text(x))  #text cleanup using function\n",
        "data.drop(columns=[\"Plot\",\"Book_genres\"], inplace=True)  #things like space, \", and all other unwanted stuffs\n",
        "\n",
        "#nltk.download('stopwords')    #to download the list of stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data['clean_plot'] = data['clean_plot'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "# print frequent words \n",
        "\n",
        "#freq_words(data['clean_plot'], 100)\n",
        "\n",
        "#using multilabelbinarizer to convert multiple labels into binaries\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "#227 unique genres\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "\n",
        "multilabel_binarizer.fit(data['genre_new'])\n",
        "\n",
        "\n",
        "\n",
        "# transform target variable\n",
        "\n",
        "y = multilabel_binarizer.transform(data['genre_new'])\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
        "\n",
        "xtrain, xval, ytrain, yval = train_test_split(data['clean_plot'], y, test_size=0.2, random_state=9)\n",
        "\n",
        "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
        "xval_tfidf = tfidf_vectorizer.transform(xval)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Binary Relevance\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Performance metric\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "lr = LogisticRegression()\n",
        "clf = OneVsRestClassifier(lr)\n",
        "\n",
        "# fit model on train data\n",
        "clf.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "# make predictions for validation set\n",
        "y_pred = clf.predict(xval_tfidf)\n",
        "\n",
        "y_pred[3]\n",
        "\n",
        "# predict probabilities\n",
        "y_pred_prob = clf.predict_proba(xval_tfidf)\n",
        "\n",
        "t = 0.2 # threshold value\n",
        "y_pred_new = (y_pred_prob >= t).astype(int)\n",
        "\n",
        "for i in range(5): \n",
        "  k = xval.sample(1).index[0] \n",
        "  print(\"Book: \", data['Book_title'][k], \"\\nPredicted genre: \", infer_tags(xval[k])), print(\"Actual genre: \",data['genre_new'][k], \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 12 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 20 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 78 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 117 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 120 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 151 is present in all training examples.\n",
            "  str(classes[c]))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 184 is present in all training examples.\n",
            "  str(classes[c]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Book:  The Fifth of March \n",
            "Predicted genre:  [()]\n",
            "Actual genre:  [\"Children's literature\", 'Fiction'] \n",
            "\n",
            "Book:  Lazarus Rising \n",
            "Predicted genre:  [('Speculative fiction',)]\n",
            "Actual genre:  ['Science Fiction', 'Speculative fiction', 'Fiction'] \n",
            "\n",
            "Book:  Iceworld \n",
            "Predicted genre:  [('Science Fiction', 'Speculative fiction')]\n",
            "Actual genre:  ['Science Fiction', 'Speculative fiction'] \n",
            "\n",
            "Book:  Seventeen Against the Dealer \n",
            "Predicted genre:  [('Fiction',)]\n",
            "Actual genre:  [\"Children's literature\", 'Young adult literature'] \n",
            "\n",
            "Book:  Religion Explained \n",
            "Predicted genre:  [()]\n",
            "Actual genre:  ['Psychology', 'Science'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRduhgVVTfye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "29ad1617-e7c9-4c89-c5e7-c5fa234ccedf"
      },
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxw2kOApSMck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfb211b5-d149-4531-a2ec-ceb400e97004"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}